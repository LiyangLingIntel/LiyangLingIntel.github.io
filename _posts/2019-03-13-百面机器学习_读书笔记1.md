---
layout:     post
title:      "剑指机器学习--模型评估"
subtitle:   机器学习方法复习笔记-1
date:       2019-03-13
author:     Lyon Ling
header-img: img/post-bg-flat-eveluation.jpg
catalog: true
mathjax: true
tags:
    - JobHunting
    - MachineLearning
    - Notes
    - 百面机器学习
---

> 没有测量，就没有科学。
>
> —— 门捷列夫

[TOC]

### 1. 评估指标的局限

#### 1.1 基本概念

* **混淆矩阵**

  |                      | Actual: Positive | Actual: Negative |
  | :------------------: | :--------------: | :--------------: |
  | **Predicted: True**  |        TP        |        FP        |
  | **Predicted: False** |        FN        |        TN        |

  * **TP**: 将正类预测为正类
  * **TN**: 将负类预测为负类
  * **FP**: 将负类预测为正类 → **误报** (Type I error).
  * **FN**: 将正类预测为负类数 → **漏报** (Type II error).
  * 记忆技巧：**P**和**N**分别表示预测结果的Positive或者Negative，而**T**和**F**则表示对预测结果的正误的判断。例如**FN**表示预测为负的这个预测是错误的，即把正类预测为负。

由混淆矩阵，我们可以得出几个评估指标

* **准确率**(Accuracy) = $\tfrac{TP+TN}{TP+FN+FP+TN}$

  针对**全集**而言， 表示预测为正的样本在所有样本中的比重

* **精确率**(Precision) = $\tfrac{TP}{TP+FP}$

  针对我们**预测结果**而言的，表示的是预测为正的样本中有多少是真正的正样本

* **召回率**(Recall) = $\tfrac{TP}{TP+FN}$

  针对我们原来的**样本**而言的，表示的是样本中的正例有多少被预测正确了。

* **F-Measure** = $\tfrac{(\alpha^2 + 1)P\times R}{\alpha^2 P + R}$

  为了解决精确率和召回率有时候会发生的冲突问题，我们引入F值来评估。

  常见的有F1值，是精确率和召回率的**调和均值**，令$\alpha = 1$则得到我们的F1值

上面的是基于分类问题的评估，而基于回归问题，我们有如下方法：

* **MAE**(Mean Absolute Error),  **l1-norm loss**

  $$MAE(y, \hat{y})=\frac{1}{n_{samples}}\sum_{i=1}^{n_{samples}}\left|y_i-\hat{y_i}\right|$$

* **MSE**(Mean Squared Error), **l2-norm loss**

  $$MSE(y, \hat{y})=\frac{1}{n_{samples}}\sum_{i=1}^{n_{samples}}(y_i-\hat{y_i})^2$$

  以及还有衍生的$RMSE$, $MAPE$ 等方法就不一一介绍了。

#### 1.2 精确率和召回率之间的权衡

为了提高precision，分类器更偏向在更有把握时才把样本标为正，这样就会使得分类更加保守，从而漏掉很多本为正的真实样本，从而导师Recall降低。

所以评估模型好坏时，不能仅仅只看Recision和Recall，最好可以绘制模型的P-R Curve。

P-R Curve以Precision作为纵轴，Recall作为横轴，如下：

<img src="https://ws1.sinaimg.cn/large/006tKfTcgy1g12etbv0tsj30ss0q0n3f.jpg" width="400" />

PR曲线上的每个点都表示某个参数配置下模型的精确率和召回率。

此外还可以用`F1 Score`和`ROC曲线`来表示模型性能。

#### 1.3 RMSE 的缺陷

首先RMSE的定义：$RMSE  = \sqrt{\frac{\sum_{i=1}^N(y_i-\hat{y_i})^2}{n}}$

实际问题中，如果存在个别离群点(Outliers)的离群成都非常大，即使数量很少，对RMSE也会产生很大的影响。

解决办法：

* 如果认定离群点是”噪声“，那么在数据预处理的时候就可以去掉

* 通过进一步的建模，把离群点的产生的机制建模进去

* 寻找更适合的指标。比如MAPE(平均绝对百分比误差)

  $$MAPE=\sum_{i=1}^N\left|\frac{y_i-\hat{y_i}}{y_i}\right|\times\frac{100}{n}$$

  MAPE相比于RMSE，把每个点的误差做了归一化，降低了个别离群点带来的绝对误差的影响。

### 2. ROC曲线

#### 2.1 基础概念

* **ROC曲线**(Receiver Operating Characteristic Curve): 横坐标表示FPR(False Positive Rate)，纵坐标表示TPR(True Positive Rate)。其中

  * $FPR=\frac{FP}{FP+TN}$, 指在所有负样本中预测为正的比率（误报率）
  *  $TPR=\frac{TP}{TP+FN}$. 指在所有正样本中预测为正的比率（召回率）

* **AUC曲线**(Area Under Curve): 即ROC曲线下面积。能够量化地反应基于ROC曲线衡量的模型性能。一般来说计算AUC只要对ROC曲线沿着横轴做积分就好了。

  ROC曲线一般都在$y=x$这条直线上方，所以AUC的值一般都在0.5～1之间，AUC越大说明模型越能把真正的正样本排在前面，分类性能越好。

#### 2.2 ROC曲线和P-R曲线有什么区别？

当正负样本分布发生变化时，ROC曲线能够基本保持不变，但是P-R曲线一般会发生比较剧烈的变化。

<img src="https://ws2.sinaimg.cn/large/006tKfTcgy1g12ep1tl1mj30u00ul4ar.jpg" width="500"/>

上图可以从一定程度说明，ROC曲线能够尽可能降低不同测试集带来的干扰，更加客观地反应模型性能。但是对这两种曲线的选择也要因问题而异。如果想要看到模型在特定测试集上的表现，P-R曲线可能就是更好的选择。

### 3. 模型评估方法

#### 3.1 有哪些主要模型评估方法，及其优缺点？

* **Holdout**

  比较简单直接的方法，把原始数据机随机划分成训练集和测试集两部分。例如70%的样本用于模型训练，30%的样本用于模型验证，包括绘制ROC曲线，计算精确率，召回率等指标评估模型性能。

* **Cross-Validation**

  * k-fold cross-validation

    把数据集分成k份，每次训练把其中一份作为测试集，其余所有作为训练集。最后把k次评估的平均指标作为评估结果。通常k取5或10

  * Leave one cross-validation

    把长度为n的数据集，每次训练取其中一个样本做测试， 其他都做训练集。最后平均每次的评估结果。

* **BootStrap**

  对总长度为n的数据集，做n次有放回的随机抽样，得到长度为n的训练集，剩下没有被抽到的数据作为测试集。

#### 3.2 BootStrap随机抽样的过程中，如果样本总数n趋向无穷大，那么没有被抽到的数据有多少？

单个样本在单次采样过程中没有被抽到的概率为$(1-\frac{1}{n})$, 在n次采样中: $(1-\frac{1}{n})^n$,  所以当n趋向于正无穷时: $\lim_{i\to\infty}(1-\frac{1}{n})^n$.

根据**重要极限**（学好高数很重要）：$\lim_{i\to\infty}(1-\frac{1}{n})^n=e$

$$\lim_{i\to\infty}(1-\frac{1}{n})^n =  \lim_{i\to\infty}{\frac{1}{(1+\frac{1}{n-1})^n}} = \frac{1}{\lim_{i\to\infty}{(1+\frac{1}{n-1})^{n-1}}}\cdot\frac{1}{\lim_{i\to\infty}{(1+\frac{1}{n-1})}} = \frac{1}{e} \simeq 0.368 $$

因此当样本数量很大时，有约36.8%的样本没有被选择过，可以作为验证集。

### 4. 超参数调优

#### 4.1 超参数调优有哪些方法？

* **超参数搜索算法包含哪些要素：**

  * 目标函数，算法需要最大化/最小化的目标，*可能是我们的loss function*
  * 搜索范围，一般需要指定搜索的上界和下界
  * 其他参数，比如搜索的步长，因算法而异

* **网格搜索**

  通过查找搜索范围内的所有点来确定最优值。如果用较大搜索范围和较小步长，有很大概率可以搜索到全局最优值，但是对计算资源需求比较高。

  实际使用中，一般先选取较大范围较大步长，确定最优解可能存在范围后再逐步缩小范围和步长。但是因为目标函数是**非凸**的，所以可能会跳过全局最优值。

* **随机搜索**

  在搜索范围内随机取样本点。只要样本点集足够大，那么就有很大概率可以找到全局最优值，或者其近似值。随机搜索一般比网格搜索快，但是其结果也没法保证。

* **贝叶斯优化算法**

  对**目标函数的形状进行学习**，找到使目标函数图像向全集最优值提升的参数。具体步骤是：

  * 根据先验分布，假设一个搜集函数
  * 每次采用新的样本点来测试目标函数，然后用这个信息来更新目标函数的先验分布
  * 算法测试由后验分布给出全局最优值可能出现的位置的点

  贝叶斯优化算法的问题在于有可能卡在局部最优点，所以算法会在”搜索“和”利用“之间找到一个平衡。即在为采样的区域**“搜索”**新的采样点，**“利用”**根据后验分布得出的全局最优值可能出现位置的采样点。

### 5. 过拟合与欠拟合

#### 5.1 怎么降低过拟合或者欠拟合的风险？

* 过拟合
  * 增加训练数据量
  * 降低模型复杂度
  * 运用正则化方法
  * 运用集成学习的方法
* 欠拟合
  * 增加新特征
  * 增加模型复杂度，增强模型的拟合能力
  * 减少正则化系数

### 6. $\alpha/\beta$ 测试

#### 6.1 为什么还要进行A/B测试？

* 离线测试不能完全避免过拟合的风险
* 离线评估无法还原真实的工作场景。一般的，离线评估无法完整包括线上使用时存在的延迟，数据丢失，标签数据缺失等情况
* 线上系统所需要的一些商业指标，在线下测试中无法被包含。比如新上线的推荐算法，线下测试关注的是模型本身的ROC曲线，P-R曲线，但是线上测试更关注的是该算法带来的用户点击率，留存时长和PV访问量的增长。

#### 6.2 如何进行A/B测试？

对用户进行分箱，分为实验组和对照组。实验组施以新模型，对照组施以旧模型。分箱过程中要注意到用户样本的独立性和无偏性。

### 7. 余弦距离

#### 7.1 基础概念

* **余弦距离**：$dist(A,B)=1-cos\angle(A,B)$

* **欧式距离**体现距离上的绝对差异，而**余弦距离**体现方向上的相对差异。比如分析用户观看行为偏好，那么比较两个用户在不同视频上的偏好，应该采用余弦距离，而对比用户在视频上花费的时间则选用欧式距离。

#### 7.2 余弦距离是严格定义的距离吗？

* 在集合中，一对元素之间存在唯一的一个$d\in\R$, 使得三条距离公理成立（正定性，对称性，三角不等性），则称d为这对元素之间的距离
* 余弦距离满足正定性和对称性，但是不满足三角不等性
* 机器学习领域中的距离，除了余弦距离，还有KL距离（Kullback-Leibler Divergence）也叫相对熵，也不符合三角不等性。









