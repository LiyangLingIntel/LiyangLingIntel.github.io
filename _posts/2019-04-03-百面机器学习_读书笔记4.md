---
layout:     post
title:      "剑指机器学习--经典算法"
subtitle:   机器学习方法复习笔记-4
date:       2019-04-03
author:     Lyon Ling
header-img: img/post-bg-jobhunting4.png
catalog: true
mathjax: true
tags:
    - JobHunting
    - MachineLearning
    - Notes
    - 百面机器学习
---

> 不忘初心，方得始终。
>
> 随着神经网络，深度学习大行其道，传统的机器学习算法似乎不再热门。但是作为基础，当没有强大算力和大量数据的时候，这些传统的方法却可以更加灵活地使用。
>
> 本文则结合问题对几种经典的分类算法进行介绍。

[TOC]

### 1. 支持向量机(Support Vector Machine)

在机器学习领域中，SVM涵盖了各个方面的知识点，也是。

#### 1.1 空间上线性可分的两类点，分别向SVM分类的超平面做投影，这些投影仍然是线性可分的吗？



### 2. 逻辑回归(Logistic Regression)



### 3. 决策树(Decision Tree)



#### 3.x 信息熵表达式的由来？

该问题来自[【知乎·D.Han的回答】](https://www.zhihu.com/question/22178202/answer/223017546)

信息论之父克劳德·香农，总结出了信息熵的三条性质：

- **单调性**，即发生概率越高的事件，其所携带的信息熵越低。

  极端案例就是“太阳从东方升起”，因为为确定事件，所以不携带任何信息量。从信息论的角度，认为这句话没有消除任何不确定性。

- **非负性**，即信息熵不能为负。

  这个很好理解，因为负的信息，即你得知了某个信息后，却增加了不确定性是不合逻辑的。

- **累加性**，即多随机事件同时发生存在的总不确定性的量度是可以表示为各事件不确定性的量度的和。

  写成公式就是：

  事件$X=A, Y=B$同时发生，且两个时间相互独立。则：

  $$P(X=A,Y=B)=P(X=A)\cdot P(Y=B)$$

  信息熵为：

  $$H(A+B)=H(A)+H(B)$$

香农从数学上，严格证明了满足上述三个条件的随机变量不确定性度量函数具有唯一形式：

$$H(X) = -C\sum_{x\in\mathbb{\chi}}p(x)logp(x)$$

其中的 ![C](https://www.zhihu.com/equation?tex=C) 为常数，我们将其归一化为 ![C=1](https://www.zhihu.com/equation?tex=C%3D1) 即得到了信息熵公式。



补充一下，如果两个事件不相互独立，那么满足$H(A+B)=H(A)+H(B)-I(A,B)$ ，其中$I(A,B)$是互信息（mutual information），代表一个随机变量包含另一个随机变量信息量的度量，这个概念在通信中用处很大。

比如一个点到点通信系统中，发送端信号为 ![X](https://www.zhihu.com/equation?tex=X) ，通过信道后，接收端接收到的信号为 ![Y](https://www.zhihu.com/equation?tex=Y) ，那么信息通过信道传递的信息量就是互信息 ![I(X,Y)](https://www.zhihu.com/equation?tex=I%28X%2CY%29) 。根据这个概念，香农推出了一个十分伟大的公式，香农公式，给出了临界通信传输速率的值，即信道容量：

$$C=Blog\big(1+\frac{S}{N}\big)$$