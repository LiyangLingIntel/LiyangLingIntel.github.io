---
layout:     post
title:      "剑指机器学习--前向神经网络"
subtitle:   机器学习方法复习笔记-6
date:       2020-11-08
author:     Leon Ling
header-style: text
catalog: true
mathjax: true
tags:
    - JobHunting
    - MachineLearning
    - Notes
    - 百面机器学习
---

> 深度前向神经网络（Deep Feedforward Network）是一种非常典型的深度学习模型。其目标就是为了拟合映射$y=f(x;\theta)$将输入的$x$转化为某种预测的输出$y$，并同时学习网络的参数$\theta$，使得模型能够达到最优的近似。
>
> 前向神经网络通常是多个函数复合表示，整体模型与一个有向无环图相关联，实际常用”链式法则“来表示函数的复合方式。常见的多层感知机，自编码器，以及卷积神经网络都是其中的一员。

[TOC]

### 1. 多层感知机和布尔函数

#### 1.1 多层感知机表示异或逻辑时最少需要几个隐含层, 仅考虑二元输入？



### 2. 深度网络中的激活函数

#### 2.1 常用的激活函数及其导数

* **Sigmoid**激活函数：$f(z)=sigmoid(z)=\frac{1}{1+e^{-z}}$
* **Sigmoid**函数倒数：$f^\prime(z)=f(z)(1-f(z))$
* **Tanh**激活函数：$f(z)=tanh(z)=\frac{e^z-e^{-z}}{e^z-e^{-z}}$
* **Tanh**函数导数：$f^\prime(z)=1-(f(z))^2$
* **ReLU**激活函数：$f(z)=ReLU(z)=max(0,z)$
* **ReLU**函数导数：$f^\prime(z)=\begin{cases}1, & z>0 \\\\ 0, & z\le0\end{cases}$

#### 2.2 为什么Sigmoid和Tanh激活函数会导致梯度消失现象

如下左图，**Sigmoid**激活函数的曲线将输入z映射到区间$(0，1)$当 $z$ 很大时，$f(z)$趋近于1；当$z$很小时，$f(z)$趋近于0。其导数在$z$很大或很小时都会趋近于0，造成梯度消失的现象。

如下右图，**Tanh**激活函数的曲线当$z$很大时，$f(z)$趋近于1；当z很小时，$f(z)$趋近于−1。其导数在$z$很大或很小时都会趋近于0，同样会出现“梯度消失”。

<img src="/img/in-post/2019-百面机器学习/20190711-mlinterview61.png">

实际上，**Tanh**激活函数相当于**Sigmoid**的平移放缩：$tanh(x)=2sigmoid(2x)-1$

#### 2.3 ReLU系列的激活函数相对于Sigmoid和Tanh激活函数的优点是什么？它们有什么局限性以及如何改进？

**优点：**

1. 从计算的角度上，**Sigmoid**和**Tanh**激活函数均需要计算指数，复杂度高，而**ReLU**只需要一个阈值即可得到激活值。
2. **ReLU**的非饱和性可以有效地解决梯度消失的问题，提供相对宽的激活
   边界。
3. **ReLU**的单侧抑制提供了网络的稀疏表达能力。

**缺点：**

**ReLU**的局限性在于其训练过程中会导致神经元死亡的问题。这是由于函数$f(z)=max(0,z)$导致负梯度在经过该**ReLU**单元时被置为0，且在之后也不被任何数据激活，即流经该神经元的梯度永远为0，不对任何数据产生响应。在实际训练中，如果学习率（Learning Rate）设置较大，会导致超过一定比例的神经元不可逆死亡，进而参数梯度无法更新，整个训练过程失败。

为解决这一问题，出现了**ReLU**的变种——**Leaky ReLU（LReLU）**：$LReLU(z)=\begin{cases}z, & z>0 \\\\az, & z\le0\end{cases}$

**ReLU**和**LReLU**的函数曲线对比如下图，**LReLU**与**ReLU**的区别在于，当$z<0$时其值不为0，而是一个斜率为$a$的线性函数，一般$a$为一个很小的正常数，这样既实现了单侧抑制，又保留了部分负梯度信息以致不完全丢失。但另一方面，$a$值的选择增加了问题难度，需要较强的人工先验或多次重复训练以确定合适的参数值。

<img src="/img/in-post/2019-百面机器学习/20190711-mlinterview62.png">

基于此，参数化的**PReLU（Parametric ReLU）**应运而生。它与**LReLU**的主要区别是将负轴部分斜率$a$作为网络中一个可学习的参数，进行反向传播训练，与其他含参数网络层联合优化。而另一个**LReLU**的变种增加了“随机化”机制，具体地，在训练过程中，斜率$a$作为一个满足某种分布的随机采样；测试时再固定下来。**Random ReLU（RReLU）**在一定程度上能起到正则化的作用。

此外，**ReLU**还有一个问题就是在$z=0$处不可导。所以后续又有人提出**Exponential Linear Unit (ELU)**: $ELU(z)=\begin{cases}z, & z>0 \\\\ \alpha (e^z-1)z, & z\le0\end{cases}$

<img src="/img/in-post/2019-百面机器学习/20190711-mlinterview63.png" width="260">

和用来解决**ELU**中$\alpha$不好确定的**Scaled Exponential Linear Unit (SELU)**: $SELU(z)=\begin{cases}z, & z>0 \\\\ \lambda\alpha (e^z-1)z, & z\le0\end{cases}$, $\alpha = 1.67326324235, \lambda = 1.05070098736$ 。

<img src="/img/in-post/2019-百面机器学习/20190711-mlinterview64.png" width="260">

这里的参数$\alpha$和$\lambda$有一套推导计算流程，可以详见作者的[GitHub](https://github.com/bioinf-jku/SNNs/blob/master/getSELUparameters.ipynb)。

### 3. 多层感知机的反向传播算法



### 4. 神经网络训练技巧

在训练大规模神经网络的时候, 我们常常遇到过拟合的问题. 为了解决过拟合, 我们常常使用

* 数据增强(Data Augmentation)
* 正则化(Regularization)
* 集成训练(Model Ensemble)
* 批量归一化(Batch Normalization)
* Dropout

#### 4.1 网络训练的时候是否可以初始化全部参数为0?

不可以.

考虑fully connected的深度网络, 同一层的任意神经元都是同构的, 并且拥有相同的输入和输出. 如果再将参数都初始化为相同的值, 那么无论前向传播还是反向传播, 取值都是完全相同的. 学习的过程没有办法打破这种对称性, 最后导致训练完成同一层下的所有参数还是相同的.

所以, 要随机初始化神经网络的参数的值, 以打破对称性. 简单来说, 可以初始化参数范围 $(-\frac{1}{\sqrt{d}},\frac{1}{\sqrt{d}})$的均匀分布, 其中 $d$ 是神经元输入数据的维度. 而Bias可以被简单设置为0, 并不会造成参数不对称的问题.

#### 4.2 为什么Dropout可以抑制过拟合?

Dropout是指在深度网络的训练中，以一定的概率随机地 “临时丢弃”一部分神经元节点。具体来讲，Dropout作用于每份小批量训练数据，由于其随机丢弃部分神经元的机制，相当于每次迭代都在训练不同结构的神经网络。类比于Bagging方法，Dropout可被认为是一种实用的大规模深度神经网络的模型集成算法。这是由于传统意义上的Bagging涉及多个模型的同时训练与测试评估，当网络与参数规模庞大时，这种集成方式需要消耗大量的运算时间与空间。Dropout在小批量级别上的操作，提供了一种轻量级的Bagging集成近似，能够实现指数级数量神经网络的训练与评测。

Dropout的具体实现中，要求某个神经元节点激活值以一定的概率p被“丢弃”，即该神经元暂时停止工作，如下图所示。因此，对于包含N个神经元节点的网络，在Dropout的作用下可看作为2N个模型的集成。这2N个模型可认为是原始网络的子网络，它们共享部分权值，并且具有相同的网络层数，而模型整体的参数数
目不变，这就大大简化了运算。对于任意神经元，每次训练中都与一组随机挑选的不同的神经元集合共同进行优化，这个过程会减弱全体神经元之间的联合适应性，减少过拟合的风险，增强泛化能力。

<img src="/img/in-post/2019-百面机器学习/20190711-mlinterview65.png">

在神经网络中应用Dropout包括训练和预测两个阶段。在训练阶段中，每个神经元节点需要增加一个概率系数，如下图所示。

<img src="/img/in-post/2019-百面机器学习/20190711-mlinterview66.png">

训练阶段又分为前向传播和反向传播两个步骤。原始网络对应的前向传播公式为：

$\begin{align}z_i^{(l+1)}&=w_i^{(l+1)}u^l+b^{(l+1)}_i,\\\\ y_i^{(l+1)}&=f(z^{(l+1)}_i).\end{align}$

应用Dropout之后，前向传播公式变为：

$\begin{align}r_i^{(l)}&\sim Bernoulli(p), \\\\  \tilde{y}^{(1)}&=r{(l)}\ast y{(l)},\\\\ z_i^{(l+1)}&=w_i^{(l+1)}\tilde{l}+b_i^{(l+1)},\\\\ y_i^{(l+1)}&=f(z^{(l+1)}_i).\end{align}$

上面的$Bernoulli$函数的作用是以概率系数$p$随机生成一个取值为0或1的向量，代表每个神经元是否需要被丢弃。如果取值为 0，则该神经元将不会计算梯度或参与后面的误差传播。

测试阶段是前向传播的过程。在前向传播的计算时，每个神经元的参数要预先乘以概率系数$p$，以恢复在训练中该神经元只有$p$的概率被用于整个神经网络的前向传播计算。

#### 4.3 Batch Norm的基本原理是什么? 在卷积网络中如何使用?

神经网络训练过程的本质是学习数据分布，如果训练数据与测试数据的分布不同将大大降低网络的泛化能力，因此我们需要在训练开始前对所有输入数据进行归一化处理。

然而随着网络训练的进行，每个隐层的参数变化使得后一层的输入发生变化，从而每一批训练数据的分布也随之改变，致使网络在每次迭代中都需要拟合不同的数据分布，增大训练的复杂度以及过拟合的风险。

批量归一化方法是针对每一批数据，在网络的每一层输入之前增加归一化处理（均值为0，标准差为1），将所有批数据强制在统一的数据分布下，即对该层的任意一个神经元（假设为第$k$维）$\hat{x}^{(k)}$采用如下公式 $\hat{x}^{(k)}=\frac{x^{(k)}-E[x^{(k)}]}{\sqrt{Var[x^{(k)}]}}$，其中$x^{(k)}$为该层第k个神经元的原始输入数据，$E[x^{(k)}]$为这一批输入数据在第$k$个神经元的均值， $\sqrt{Var[x^{(k)}]}$为这一批数据在第$k$个神经元的标准差。

批量归一化可以看作在每一层输入和上一层输出之间加入了一个新的计算层，对数据的分布进行额外的约束，从而增强模型的泛化能力。但是批量归一化同时也降低了模型的拟合能力，归一化之后的输入分布被强制为0均值和1标准差。以Sigmoid激活函数为例，批量归一化之后数据整体处于函数的非饱和区域，只包含线性变换，破坏了之前学习到的特征分布。为了恢复原始数据分布，具体实现中引入了变换重构以及可学习参数 $\gamma$ 和 $\beta$：$y^{(k)}=\gamma^{(k)}\hat{x}^{(k)}+\beta^{(k)}$，其中$\gamma^{(k)}$和$\beta^{(k)}$分别为输入数据分布的方差和偏差。

对于一般的网络，不采用批量归一化操作时，这两个参数高度依赖前面网络学习到的连接权重（对应复杂的非线
性）。而在批量归一化操作中，$\gamma$和$\beta$变成了该层的学习参数，仅用两个参数就可以恢复最优的输入数据分布，与之前网络层的参数解耦，从而更加有利于优化的过程，提高模型的泛化能力。

完整的批量归一化网络层的前向传导过程公式如下：$$\begin{array}\mu_B\leftarrow\frac{1}{m}\Sigma^m_{i=1}(x_{i}-\mu_B)^2,\\\\ \sigma^2_B\leftarrow\frac{1}{m}\Sigma^m_{i=1}(x_i-\mu_B)^2,\\\\ \hat{x}_i\leftarrow\frac{x_i-\mu_Beta}{\sqrt{\sigma^2_B+\epsilon}},\\\\ y_i\leftarrow\gamma \hat{x}_i+\beta\equiv BN_{\gamma,\beta}(x_i)\end{array}$$

批量归一化在卷积神经网络中应用时，需要注意卷积神经网络的参数共享机制。每一个卷积核的参数在不同位置的神经元当中是共享的，因此也应该被一起归一化。具体实现中，假设网络训练中每一批包含$b$个样本，由一个卷积核生成的特征图的宽高分别为$w$和$h$，则每个特征图所对应的全部神经元个数为$b×w×h$；利用这些神经元对应的所有输入数据，我们根据一组待学习的参数γ和β对每个输入数据进行批量归一化操作。如果有f个卷积核，就对应f个特征图和f组不同的$γ$和$β$参数。

### 5. 卷积神经网络

**卷积神经网络（Convolutional Neural Networks，CNN）**也是一种前馈神经网络，其特点是每层的神经元节点只响应前一层局部区域范围内的神经元（全连接网络中每个神经元节点响应前一层的全部节点）。

一个深度卷积神经网络模型通常由若干卷积层叠加若干全连接层组成，中间也包含各种非线性操作以及池化操
作。卷积神经网络同样可以使用反向传播算法进行训练，相较于其他网络模型，卷积操作的参数共享特性使得需要优化的参数数目大大缩减，提高了模型的训练效率以及可扩展性。由于卷积运算主要用于处理类网格结构的数据，因此对于时间序列以及图像数据的分析与识别具有显著优势。

#### 5.1 卷积的操作的本质特性是什么? 

稀疏交互(Sparse Interaction)和参数共享(Parameter Sharing).

**稀疏交互**

在传统神经网络中，网络层之间输入与输出的连接关系可以由一个权值参数矩阵来表示，其中每个单独的参数值都表示了前后层某两个神经元节点之间的交互。对于全连接网络，任意一对输入与输出神经元之间都产生交互，形成稠密的连接结构，如图所示，神经元$s_i$与输入的所有神经元$x_j$均有连接。

<img src="/img/in-post/2019-百面机器学习/20190711-mlinterview69.png" width="350" >

而在卷积神经网络中，卷积核尺度远小于输入的维度，这样每个输出神经元仅与前一层特定局部区域内的神经元存在连接权重（即产生交互），我们称这种特性为**稀疏交互**，如下图所示。可以看到与稠密的连接结构不同，神经元$s_i$仅与前一层中的$x_{i−1}$、$x_i$和$x_{i+1}$相连。具体来讲，假设网络中相邻两层分别具有$m$个输入和$n$个输出，全连接网络中的权值参数矩阵将包含$m×n$个参数。对于稀疏交互的卷积网络，如果限定每个输出与前一层神经元的连接数为k，那么该层的参数总量为$k×n$。在实际应用中，一般$k$值远小于$m$就可以取得较为可观的效果；而此时优化过程的时间复杂度将会减小几个数量级，过拟合的情况也得到了较好的改善。

<img src="/img/in-post/2019-百面机器学习/20190711-mlinterview610.png" width="350" >

稀疏交互的物理意义是，通常图像、文本、语音等现实世界中的数据都具有局部的特征结构，我们可以先学习局部的特征，再将局部的特征组合起来形成更复杂和抽象的特征。

以人脸识别为例，最底层的神经元可以检测出各个角度的边缘特征（见图（a））；位于中间层的神经元可以将边缘组合起来得到眼睛、鼻子、嘴巴等复杂特征（见图（b））；最后，位于上层的神经元可以根据各器官的组合检测出人脸的特征（见图（c））。

<img src="/img/in-post/2019-百面机器学习/20190711-mlinterview611.png" >

**参数共享**

参数共享是指在同一个模型的不同模块中使用相同的参数，它是卷积运算的固有属性。全连接网络中，计算每层的输出时，权值参数矩阵中的每个元素只作用于某个输入元素一次；而在卷积神经网络中，卷积核中的每一个元素将作用于每一次局部输入的特定位置上。根据参数共享的思想，我们只需要学习一组参数集合，而不需要针对每个位置的每个参数都进行优化，从而大大降低了模型的存储需求。

参数共享的物理意义是使得卷积层具有平移等变性。

假如图像中有一只猫，那么无论它出现在图像中的任何位置，我们都应该将它识别为猫，也就是说神经网络的输出对于平移变换来说应当是等变的。特别地，当函数 $f (x)$ 与 $g(x)$ 满足 $f(g(x))=g( f (x))$ 时，我们称 $f (x)$ 关于变换 $g$ 具有等变性。将 $g$ 视为输入的任意平移函数，令 $I$ 表示输入图像（在整数坐标上的灰度值函数），平移变换后得到 $I’=g(I)$。例如，我们把猫的图像向右移动 $l$ 像素，满足 $I’(x,y)=I(x−l,y)$。我们令 $f$ 表示卷积函数，根据其性质，我们很容易得到 $g(f(I))=f(I’)=f (g(I))$。也就是说，在猫的图片上先进行卷积，再向右平移l像素的输出，与先将图片向右平移l像素再进行卷积操作的输出结果是相等的。

#### 5.2 常用的池化操作有哪些? 池化的作用?

常用的池化操作主要针对非重叠区域，包括均值池化（mean pooling）、最大池化（max pooling）等。

**均值池化**通过对邻域内特征数值求平均来实现，能够抑制由于邻域大小受限造成估计值方差增大的现象，特点是对背景的保留效果更好。

**最大池化**则通过取邻域内特征的最大值来实现，能够抑制网络参数误差造成估计均值偏移的现象，特点是更好地提取纹理信息。

池化操作的本质是降采样。例如，我们可以利用最大池化将4×4的矩阵降采样为2×2的矩阵，如图所示。图中的池化操作窗口大小为2×2，步长为2。每次在2×2大小的窗口上进行计算，均值池化是求窗口中元素的均值，最大池化则求窗口中元素的最大值；然后将窗口向右或向下平移两格，继续操作。

<img src="/img/in-post/2019-百面机器学习/20190711-mlinterview67.png" width="400">

此外，特殊的池化方式还包括对相邻重叠区域的池化以及空间金字塔池化。

相邻重叠区域的池化，顾名思义，是采用比窗口宽度更小的步长，使得窗口在每次滑动时存在重叠的区域。空间金字塔池化主要考虑了多尺度信息的描述，例如同时计算$1×1$、$2×2$、$4×4$的矩阵的池化并将结果拼接在一起作为下一网络层的输入。

池化操作除了能显著降低参数量外，还能够保持对平移、伸缩、旋转操作的不变性。

**平移不变性**是指输出结果对输入的小量平移基本保持不变。例如，输入为$(1,5,3)$，最大池化将会取5，如果将输入右移一位得到$(0,1,5)$，输出的结果仍将为5。

**对伸缩的不变性**（一般称为**尺度不变性**）可以这样理解，如果原先神经元在最大池化操作之后输出5，那么在经过伸缩（尺度变换）之后，最大池化操作在该神经元上很大概率的输出仍然是5。因为神经元感受的是邻域输入的最大值，而并非某一个确定的值。

**旋转不变性**可以参照下图。图中的神经网络由3个学得的过滤器和一个最大池化层组成。这3个过滤器分别学习到不同旋转方向的“5”。当输入中出现“5”时，无论进行何种方向的旋转，都会有一个对应的过滤器与之匹配并在对应的神经元中引起大的激活。最终，无论哪个神经元获得了激活，在经过最大池化操作之后输出都会具有大的激活。

<img src="/img/in-post/2019-百面机器学习/20190711-mlinterview68.png" width="450">

#### 5.3 卷积神经网络在文本分类任务中怎么使用?

对于文本来说，局部特征就是由若干单词组成的滑动窗口，类似于N-gram。卷积神经网络的优势在于能够自动地对Ngram特征进行组合和筛选，获得不同抽象层次的语义信息。由于在每次卷积中采用了共享权重的机制，因此它的训练速度相对较快，在实际的文本分类任务中取得了非常不错的效果。

下图是一个用卷积神经网络模型进行文本表示，并最终用于文本分类的网络结构。

<img src="/img/in-post/2019-百面机器学习/20190711-mlinterview612.png" >

1. 输入层是一个$N×K$的矩阵，其中$N$为文章所对应的单词总数，$K$是每个词对应的表示向量的维度。每个词的$K$维向量可以是预先在其他语料库中训练好的，也可以作为未知的参数由网络训练得到。这两种方法各有优势，一方面，预先训练的词嵌入可以利用其他语料库得到更多的先验知识；另一方面，由当前网络训练的词向量能够更好地抓住与当前任务相关联的特征。因此，图中的输入层实际采用了两个通道的形式，即有两个$N×K$的输入矩阵，其中一个用预先训练好的词嵌入表达，并且在训练过程中不再发生变化；另外一个也由同样的方式初始化，但是会作为参数，随着网络的训练过程发生改变。
2. 第二层为卷积层。在输入的$N×K$维矩阵上，我们定义不同大小的滑动窗口进行卷积操作: $c_i=f(w\cdot x_{i:i+h-1}+b)$，其中$x_{i:i+h−1}$代表由输入矩阵的第i行到第$i+h−1$行所组成的一个大小为$h×K$的滑动窗口，$w$为$K×h$维的权重矩阵，$b$为偏置参数。假设$h$为$3$，则每次在$2×K$的滑动窗口上进行卷积，并得到$N−2$个结果，再将这$N−2$个结果拼接起来得到$N−2$维的特征向量。每一次卷积操作相当于一次特征向量的提取，通过定义不同的滑动窗口，就可以提取出不同的特征向量，构成卷基层的输出。
3. 第三层为池化层，比如图中所示的网络采用了1-Max池化，即为从每个滑动窗口产生的特征向量中筛选出一个最大的特征，然后将这些特征拼接起来构成向量表示。也可以选用K-Max池化（选出每个特征向量中最大的K个特征），或者平均池化（将特征向量中的每一维取平均）等，达到的效果都是将不同长度的句子通过池化得到一个定长的向量表示。
4. 得到文本的向量表示之后，后面的网络结构就和具体的任务相关了。本例中展示的是一个文本分类的场景，因此最后接入了一个全连接层，并使用Softmax激活函数输出每个类别的概率。

### 6. 残差网络

To be updated.