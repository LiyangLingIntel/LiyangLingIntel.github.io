---
layout:     post
title:      "剑指机器学习--前向神经网络"
subtitle:   机器学习方法复习笔记-6
date:       2020-11-08
author:     Leon Ling
header-style: text
catalog: true
mathjax: true
tags:
    - JobHunting
    - MachineLearning
    - Notes
    - 百面机器学习
---

> 深度前向神经网络（Deep Feedforward Network）是一种非常典型的深度学习模型。其目标就是为了拟合映射$y=f(x;\theta)$将输入的$x$转化为某种预测的输出$y$，并同时学习网络的参数$\theta$，使得模型能够达到最优的近似。
>
> 前向神经网络通常是多个函数复合表示，整体模型与一个有向无环图相关联，实际常用”链式法则“来表示函数的复合方式。常见的多层感知机，自编码器，以及卷积神经网络都是其中的一员。

[TOC]

### 1. 多层感知机和布尔函数

#### 1.1 多层感知机表示异或逻辑时最少需要几个隐含层, 仅考虑二元输入？



### 2. 深度网络中的激活函数

#### 2.1 常用的激活函数及其导数

* **Sigmoid**激活函数：$f(z)=sigmoid(z)=\frac{1}{1+e^{-z}}$
* **Sigmoid**函数倒数：$f^\prime(z)=f(z)(1-f(z))$
* **Tanh**激活函数：$f(z)=tanh(z)=\frac{e^z-e^{-z}}{e^z-e^{-z}}$
* **Tanh**函数导数：$f^\prime(z)=1-(f(z))^2$
* **ReLU**激活函数：$f(z)=ReLU(z)=max(0,z)$
* **ReLU**函数导数：$f^\prime(z)=\begin{cases}1, & z>0 \\\\ 0, & z\le0\end{cases}$

#### 2.2 为什么Sigmoid和Tanh激活函数会导致梯度消失现象

如下左图，**Sigmoid**激活函数的曲线将输入z映射到区间$(0，1)$当 $z$ 很大时，$f(z)$趋近于1；当$z$很小时，$f(z)$趋近于0。其导数在$z$很大或很小时都会趋近于0，造成梯度消失的现象。

如下右图，**Tanh**激活函数的曲线当$z$很大时，$f(z)$趋近于1；当z很小时，$f(z)$趋近于−1。其导数在$z$很大或很小时都会趋近于0，同样会出现“梯度消失”。

<img src="/img/in-post/2019-百面机器学习/20190711-mlinterview61.png">

实际上，**Tanh**激活函数相当于**Sigmoid**的平移放缩：$tanh(x)=2sigmoid(2x)-1$

#### 2.3 ReLU系列的激活函数相对于Sigmoid和Tanh激活函数的优点是什么？它们有什么局限性以及如何改进？

**优点：**

1. 从计算的角度上，**Sigmoid**和**Tanh**激活函数均需要计算指数，复杂度高，而**ReLU**只需要一个阈值即可得到激活值。
2. **ReLU**的非饱和性可以有效地解决梯度消失的问题，提供相对宽的激活
   边界。
3. **ReLU**的单侧抑制提供了网络的稀疏表达能力。

**缺点：**

**ReLU**的局限性在于其训练过程中会导致神经元死亡的问题。这是由于函数$f(z)=max(0,z)$导致负梯度在经过该**ReLU**单元时被置为0，且在之后也不被任何数据激活，即流经该神经元的梯度永远为0，不对任何数据产生响应。在实际训练中，如果学习率（Learning Rate）设置较大，会导致超过一定比例的神经元不可逆死亡，进而参数梯度无法更新，整个训练过程失败。

为解决这一问题，出现了**ReLU**的变种——**Leaky ReLU（LReLU）**：$LReLU(z)=\begin{cases}z, & z>0 \\\\az, & z\le0\end{cases}$

**ReLU**和**LReLU**的函数曲线对比如下图，**LReLU**与**ReLU**的区别在于，当$z<0$时其值不为0，而是一个斜率为$a$的线性函数，一般$a$为一个很小的正常数，这样既实现了单侧抑制，又保留了部分负梯度信息以致不完全丢失。但另一方面，$a$值的选择增加了问题难度，需要较强的人工先验或多次重复训练以确定合适的参数值。

<img src="/img/in-post/2019-百面机器学习/20190711-mlinterview62.png">

基于此，参数化的**PReLU（Parametric ReLU）**应运而生。它与**LReLU**的主要区别是将负轴部分斜率$a$作为网络中一个可学习的参数，进行反向传播训练，与其他含参数网络层联合优化。而另一个**LReLU**的变种增加了“随机化”机制，具体地，在训练过程中，斜率$a$作为一个满足某种分布的随机采样；测试时再固定下来。**Random ReLU（RReLU）**在一定程度上能起到正则化的作用。

此外，**ReLU**还有一个问题就是在$z=0$处不可导。所以后续又有人提出**Exponential Linear Unit (ELU)**: $ELU(z)=\begin{cases}z, & z>0 \\\\ \alpha (e^z-1)z, & z\le0\end{cases}$

<img src="/img/in-post/2019-百面机器学习/20190711-mlinterview63.png" width="260">

和用来解决**ELU**中$\alpha$不好确定的**Scaled Exponential Linear Unit (SELU)**: $SELU(z)=\begin{cases}z, & z>0 \\\\ \lambda\alpha (e^z-1)z, & z\le0\end{cases}$, $\alpha = 1.67326324235, \lambda = 1.05070098736$ 。

<img src="/img/in-post/2019-百面机器学习/20190711-mlinterview64.png" width="260">

这里的参数$\alpha$和$\lambda$有一套推导计算流程，可以详见作者的[GitHub](https://github.com/bioinf-jku/SNNs/blob/master/getSELUparameters.ipynb)。

### 3. 多层感知机的反向传播算法



### 4. 神经网络训练技巧

在训练大规模神经网络的时候, 我们常常遇到过拟合的问题. 为了解决过拟合, 我们常常使用

* 数据增强(Data Augmentation)
* 正则化(Regularization)
* 集成训练(Model Ensemble)
* 批量归一化(Batch Normalization)
* Dropout

#### 4.1 网络训练的时候是否可以初始化全部参数为0?

不可以.

考虑fully connected的深度网络, 同一层的任意神经元都是同构的, 并且拥有相同的输入和输出. 如果再将参数都初始化为相同的值, 那么无论前向传播还是反向传播, 取值都是完全相同的. 学习的过程没有办法打破这种对称性, 最后导致训练完成同一层下的所有参数还是相同的.

所以, 要随机初始化神经网络的参数的值, 以打破对称性. 简单来说, 可以初始化参数范围 $(-\frac{1}{\sqrt{d}},\frac{1}{\sqrt{d}})$的均匀分布, 其中 $d$ 是神经元输入数据的维度. 而Bias可以被简单设置为0, 并不会造成参数不对称的问题.

#### 4.2 为什么Dropout可以抑制过拟合?

Dropout是指在深度网络的训练中，以一定的概率随机地 “临时丢弃”一部分神经元节点。具体来讲，Dropout作用于每份小批量训练数据，由于其随机丢弃部分神经元的机制，相当于每次迭代都在训练不同结构的神经网络。类比于Bagging方法，Dropout可被认为是一种实用的大规模深度神经网络的模型集成算法。这是由于传统意义上的Bagging涉及多个模型的同时训练与测试评估，当网络与参数规模庞大时，这种集成方式需要消耗大量的运算时间与空间。Dropout在小批量级别上的操作，提供了一种轻量级的Bagging集成近似，能够实现指数级数量神经网络的训练与评测。

Dropout的具体实现中，要求某个神经元节点激活值以一定的概率p被“丢弃”，即该神经元暂时停止工作，如下图所示。因此，对于包含N个神经元节点的网络，在Dropout的作用下可看作为2N个模型的集成。这2N个模型可认为是原始网络的子网络，它们共享部分权值，并且具有相同的网络层数，而模型整体的参数数
目不变，这就大大简化了运算。对于任意神经元，每次训练中都与一组随机挑选的不同的神经元集合共同进行优化，这个过程会减弱全体神经元之间的联合适应性，减少过拟合的风险，增强泛化能力。

<img src="/img/in-post/2019-百面机器学习/20190711-mlinterview65.png">

在神经网络中应用Dropout包括训练和预测两个阶段。在训练阶段中，每个神经元节点需要增加一个概率系数，如下图所示。

<img src="/img/in-post/2019-百面机器学习/20190711-mlinterview66.png">

训练阶段又分为前向传播和反向传播两个步骤。原始网络对应的前向传播公式为：

$\begin{align}z_i^{(l+1)}&=w_i^{(l+1)}u^l+b^{(l+1)}_i,\\\\ y_i^{(l+1)}&=f(z^{(l+1)}_i).\end{align}$

应用Dropout之后，前向传播公式变为：

$\begin{align}r_i^{(l)}&\sim Bernoulli(p), \\\\  \tilde{y}^{(1)}&=r{(l)}\ast y{(l)},\\\\ z_i^{(l+1)}&=w_i^{(l+1)}\tilde{l}+b_i^{(l+1)},\\\\ y_i^{(l+1)}&=f(z^{(l+1)}_i).\end{align}$

上面的$Bernoulli$函数的作用是以概率系数$p$随机生成一个取值为0或1的向量，代表每个神经元是否需要被丢弃。如果取值为 0，则该神经元将不会计算梯度或参与后面的误差传播。

测试阶段是前向传播的过程。在前向传播的计算时，每个神经元的参数要预先乘以概率系数$p$，以恢复在训练中该神经元只有$p$的概率被用于整个神经网络的前向传播计算。

#### 4.3 Batch Norm的基本原理是什么? 在卷积网络中如何使用?

神经网络训练过程的本质是学习数据分布，如果训练数据与测试数据的分布不同将大大降低网络的泛化能力，因此我们需要在训练开始前对所有输入数据进行归一化处理。

然而随着网络训练的进行，每个隐层的参数变化使得后一层的输入发生变化，从而每一批训练数据的分布也随之改变，致使网络在每次迭代中都需要拟合不同的数据分布，增大训练的复杂度以及过拟合的风险。

批量归一化方法是针对每一批数据，在网络的每一层输入之前增加归一化处理（均值为0，标准差为1），将所有批数据强制在统一的数据分布下，即对该层的任意一个神经元（假设为第$k$维）$\hat{x}^{(k)}$采用如下公式 $\hat{x}^{(k)}=\frac{x^{(k)}-E[x^{(k)}]}{\sqrt{Var[x^{(k)}]}}$，其中$x^{(k)}$为该层第k个神经元的原始输入数据，$E[x^{(k)}]$为这一批输入数据在第$k$个神经元的均值， $\sqrt{Var[x^{(k)}]}$为这一批数据在第$k$个神经元的标准差。

批量归一化可以看作在每一层输入和上一层输出之间加入了一个新的计算层，对数据的分布进行额外的约束，从而增强模型的泛化能力。但是批量归一化同时也降低了模型的拟合能力，归一化之后的输入分布被强制为0均值和1标准差。以Sigmoid激活函数为例，批量归一化之后数据整体处于函数的非饱和区域，只包含线性变换，破坏了之前学习到的特征分布。为了恢复原始数据分布，具体实现中引入了变换重构以及可学习参数 $\gamma$ 和 $\beta$：$y^{(k)}=\gamma^{(k)}\hat{x}^{(k)}+\beta^{(k)}$，其中$\gamma^{(k)}$和$\beta^{(k)}$分别为输入数据分布的方差和偏差。

对于一般的网络，不采用批量归一化操作时，这两个参数高度依赖前面网络学习到的连接权重（对应复杂的非线
性）。而在批量归一化操作中，$\gamma$和$\beta$变成了该层的学习参数，仅用两个参数就可以恢复最优的输入数据分布，与之前网络层的参数解耦，从而更加有利于优化的过程，提高模型的泛化能力。

完整的批量归一化网络层的前向传导过程公式如下：$$\begin{array}\mu_B\leftarrow\frac{1}{m}\Sigma^m_{i=1}(x_{i}-\mu_B)^2,\\\\ \sigma^2_B\leftarrow\frac{1}{m}\Sigma^m_{i=1}(x_i-\mu_B)^2,\\\\ \hat{x}_i\leftarrow\frac{x_i-\mu_Beta}{\sqrt{\sigma^2_B+\epsilon}},\\\\ y_i\leftarrow\gamma \hat{x}_i+\beta\equiv BN_{\gamma,\beta}(x_i)\end{array}$$

批量归一化在卷积神经网络中应用时，需要注意卷积神经网络的参数共享机制。每一个卷积核的参数在不同位置的神经元当中是共享的，因此也应该被一起归一化。具体实现中，假设网络训练中每一批包含$b$个样本，由一个卷积核生成的特征图的宽高分别为$w$和$h$，则每个特征图所对应的全部神经元个数为$b×w×h$；利用这些神经元对应的所有输入数据，我们根据一组待学习的参数γ和β对每个输入数据进行批量归一化操作。如果有f个卷积核，就对应f个特征图和f组不同的$γ$和$β$参数。

### 5. 卷积神经网络



#### 5.1 卷积的操作的本质特性是什么? 

稀疏交互(Sparse Interaction)和参数共享(Parameter Sharing).

* **稀疏交互**


* **参数共享**



#### 5.2 常用的池化操作有哪些? 池化的作用?



#### 5.3 卷积神经网络在文本分类任务中怎么使用?



### 6. 残差网络