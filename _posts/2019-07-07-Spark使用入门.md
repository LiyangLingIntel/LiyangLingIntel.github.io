---
layout:     post
title:      "Spark使用入门"
subtitle:   从零开始学Spark
date:       2019-07-07
author:     Lyon Ling
header-img: img/post-bg-spark.png
catalog: true
mathjax: false
tags:
    - Tutorial
	- BigData
	- Spark
---

> 之前在科大学习的时候学过一段时间`Spark`，但是仅限于`Spark`本身,而且只是方法上的实践，来到公司后结合了完整的Hadoop生态，发现对很多地方还是一脸懵逼的。
>
> 这里先结合公司第一次技术培训，整理一些Spark日常使用的一些问题，以供以后参考。

[TOC]

## 1. Spark 的版本

|   版本   | 区别一                                                     | 区别二                                                       | 区别三                           |
| :------: | :--------------------------------------------------------- | :----------------------------------------------------------- | :------------------------------- |
| `Spark1` | 主要为`SparkContext`，自动引用为`sc`                       | `RDD`弹性分布式数据集                                        | MLlib  API                 + RDD |
| `Spark2` | 主要为`SparkSession`，自动引用为`spark`； `sc`需要额外定义 | `Dataframe`基于`RDD`做了高级封装，原则上`RDD`在`Spark2`会下线 | ML API + DataFrame               |

## 2. Why Spark

* **Pros**：
  * Spark是基于内存计算的分布式框架，会加速所有计算（无论是否是Spark的原生程序）。
  * Spark在大规模计算的时候，速度会比Hive快出多个数量级。
  * Spark操作相比Hive操作工程化更强，可以用简单的方法实现复杂的逻辑。简单来说，Spark编程是在写程序，而Hive编程更像是在写脚本。
* **Cons**：
  * Spark模型为了大规模计算的需求，精度会低于单机模型，同样的学习率，精度可能会比`sklearn`低。
    关于这一点，我在做 [Spark DBSCAN](https://github.com/lyonling/SparkDBSCAN) 项目时体会比较深刻。虽然最终时间效率提高很多，但是同样的参数，聚类效果却总是比单机差一点点。

## 3. Spark 启动问题

### 3.1 常用启动选项

```shell
$ pyspark --jars $mmljars				# 加载指定jar包
					--py-files ./test.py	# or ./test.pyc, 支持pycfile，在一定程度上做到代码信息保密
					--driver-cores 2			# driver的core数
					--driver-memory 2G		# driver的内存大小
					--num-executors 4			# worker数量
					--executor-cores 2		# 每个worker的core数量
					--executor-memory 6G	# 每个worker的内存
					--queue queue_1234_5	# 队列
```

* Total Worker Memory = `num-executors`*`executor-memory`

* Total Memory = Total Worker Memory + Total Driver Memory

这里 core:memory 一般和机器物理配比相一致，比如物理机器共有8个核心对应64G的内存，那么做配置的时候就应该按照 1:8 去配置 core:memory。

但是配比是灵活的，在一些情况下也要做相应调整：

1. 推送数据时，数据量很大，但是需要控制并发量：
   这个时候就要控制中等数量core，增大memory，比如3个核心，20G内存
2. 想要使用的算法在`sklearn`里面：
   因为`sklearn`只支持单节点的并行运算，但是不支持分布式计算。
   所以需要减少worker的内存，增大driver的内存，直接通过`collect()`把数据分布式拉到driver上处理。
   但是除非需要反复使用这些数据，否则不建议这样做。

### 3.2 启动失败

#### 3.2.1 `py4j`服务异常

* 先看报错，查看具体问题是什么
* 使用`jps`命令，查看是否已经正确启动相关`java`服务
* 去`spark`安装目录检查配置信息是否正确

#### 3.2.2 节点OOM(Out of Memory)

潜在原因：

* 把大量数据通过`collect()`, `takeSample()`(Spark1)拉到了driver节点，导致内存溢出
* worker资源分配不够
* 数据倾斜

解决方案：

* 通过报错Error的上下语境，找到发生错误的机器的名字和ip，去查看对应机器的container jvm日志。

### 3.2.3 其他问题

在实际项目生产过程中也会因为意外，和操作不够正规出现各种奇怪的问题，这里简单举例。

* Fail to initialize AM container: Hadoop job information for stage-1: # mappers 0, # reducers 0.
  这个是因为物理集群磁盘故障，无法访问出错，但是报错不一定能明确指出，需要运维人员细心推理。
* Operational Error: database or disk is full.
  这个是因为把大量数据没有按照标准放到了机器物理存储很小的分区上，导致内存满了文件读写失败出错。
* 分布式机器python版本不完全一致，也会导致机器运行概率性出错。

### 4. 数据倾斜

做大数据处理是很容易遇到的一个问题就是数据倾斜，其分为两大类：

#### 4.1 计算倾斜

因为计算操作不规范导致产生了大量脏数据。

实例: 

现在有数据量都为$10^8$的两张表A、B做`join`操作，其中A和B都有10%的主键为空(null/00000000)，这样的情况下会产生$10^7*10^7$的脏数据，无法写出。从`Hive`层面具体表现为，`mapper`跑完后，`recucer`会卡在99%。

所以为了避免这样的问题，每次关联前都要判断主键是否非空并且去重。同样会产生计算倾斜的方法还有如`group by`, `count distinct` 使用时需要特别注意。

此外，在多表join时尽量不要写太复杂的SQL语句，容易出问题而且难debug。但是也有特殊情况，比如表A`left join`多张表，这事`Hive`会产生多个`mapper`同时计算多个stage，提高了效率。

#### 4.2 存储倾斜

往往是计算倾斜引发的难以发现的问题。

实例：

`Spark`往`HBase`上推数据，一共6个worker，4个成功，2个总是失败。

通过命令`hdfs -ls /`发现有两个特别大的文件，和4个特别小的文件。计算倾斜时，Spark会偏向把相似的数据放在同一个节点工作。

所以解决办法，在`Spark`层面，读取数据需要做repartition的操作，比如自定义partitioner或者使用range partitioner去替换默认的hash partitioner等。

## 5. Hadoop 生态

最后，想要全面认识`Spark`生态或者说是`Hadoop`生态，我们需要对生态成分中的每一环节都有一定了解，包括`Hive`, `YARN`, `Zookeeper`, `Kafka`等分别是什么，在`Hadoop`生态中扮演什么样的角色。

关于这一点，我在当时在阿里云培训的时候做了相关的整理，详情可以看 [Alibaba Cloud Training (Day1)](https://lyonling.github.io/2019/01/24/Alibaba-Cloud-Training-Day1/)。这里因为最近的培训也提到了一些内容，所以这边我又整理了一遍，以作巩固。

Hadoop 是一个能够对大量数据进行分布式处理的软件框架。具有可靠、高效、可伸缩的特点。框架内部包含很多模块，对不同的任务目标具有不同处理模块，接下来一一做解释。

<img src="https://img-blog.csdn.net/2018032606192381?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dkcjIwMDM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" />

1. HDFS： 对应其前身Google File System 解决的问题：分布式的文件存储系统。文件非常多单机无法将解决，分布式的文件存储系统任务在于在多个系统上通过调度，将小碎片文件合成大的文件。

2. MapReduce：是算法思想——拆分、合并多个任务，例如数据求和 前身是Google的MapReduce论文

3. Hue / Pig：第一代的程序框架，作用：在hadoop上跑MapReduce
   问题：程序比较难写

4. Hive：数据仓库。写的程序语法与SQL相似，将SQL转译成MapReduce程序。
   Hive数据处理基于硬盘，核心工作方法是MapReduce。

5. Spark：数据处理基于内存，计算单位是RDD，计算数据全部载到内存。

   * 回到正常的分布式编程加快，但存在问题内存不够，需要人工释放和固化(`cache()`和`persist()`)

   * DAG（有向无环图）+ 惰性计算。

     * Hive是顺序执行，逐一返回结果，而spark 基于DAG可以做些预先的推理。

     * 例子：
       Hive上做表连接操作，join完成后生成schema；
       Spark：`AB—>C, DE-->F, CF—>G`, `G`的schema在transformation定义好的时候就在Spark internal内生成，但实际计算只有在遇到action操作(`count()`，`saveAsTable()`）才会真正计算全过程。

     * 问题：

       伪action：比如`show()`，虽然很快出来结果，但只是跑出来需要show的部分结果。

   * Spark部署模式：

     * 单机（standalone），伪分布式，基于mesos。只是不同的资源管理模式。

   * Spark 1—>2的变化

     1. RDD —> Dateframe
     2. SparkContex —> SparkSession
     3. MLlib —> ML & 若干语法变化

6. YARN / Mesos：yarn本来是spark内部的资源管理调度器，spark1到spark2孵化成顶级项目，服务方向拓展，也服务hadoop的其他模块。MapReduce的资源管理也由YARN来管，然后mesos就逐渐被淘汰。

7. ZooKeeper： 管理Hadoop生态内部所有服务（因为Apache内部的所有服务以动物代表，所以顾名思义）

8. Kafka： 消息队列， 存储非结构化数据。通常用来做一些日志分析。
   Redis：内存型消息队列。类似于Spark和Hive, Redis也为Kafka提供服务。

9. Hbase：数据仓库。与Hive是并行关系。Hive是行存储，hbase是列存储。
   列存储，优点是搜索信息快，针对列做计算快。缺点是，行计算非常慢，需要先做类似join操作。
   HBase容量比Hive更大，常用来推送数据（更新数据库，在Hive上算好，在推送到HBase上做查找操作）
   问题是会丢失一定少量数据。

10. Elastic Search：分布式弹性搜索引擎（框架），快速搜索。预先给所有字段建立索引。

11. Sqoop： SQL to Hadoop，通过MapReduce任务，让数据在传统rdb和Hive等分布式大数据存储之间流通。

12. Oozie：作业流调度引擎，实现schedule的功能，跑定时任务用的。

13. Flume：日志收集工具。

14. Flink：针对流数据和批数据的分布式处理引擎。

15. Drool：分布式的支持实时分析的数据存储系统。

### 5.1 Yarn

YARN是一个资源管理、任务调度的框架，主要包含三大模块：ResourceManager（RM）、NodeManager（NM）、ApplicationMaster（AM）。

* ResourceManager负责所有资源的监控、分配和管理；
* ApplicationMaster负责每一个具体应用程序的调度和协调；
* NodeManager负责每一个节点的维护。

对于所有的applications，RM拥有绝对的控制权和对资源的分配权。而每个AM则会和RM协商资源，同时和NodeManager通信来执行和监控task。

