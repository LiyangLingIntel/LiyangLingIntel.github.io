---
layout:     post
title:      Notes on Deep Learning
subtitle:   学习Deep Learning中遇到的一些小问题汇总[持续更新]
date:       2019-01-07
author:     Lyon Ling
header-img: img/post-bg-os-metro.jpg
catalog: true
    - Deep Learning
    - Note
---



#### 为什么神经网络的激活函数必须是非线性函数？

神经网络的激活函数必须使用非线性函数。换句话说，激活函数不能使 用线性函数。为什么不能使用线性函数呢?因为使用线性函数的话，加深神 经网络的层数就没有意义了。

线性函数的问题在于，不管如何加深层数，总是存在与之等效的“无隐藏层的神经网络”。为了具体地(稍微直观地)理解这一点，我们来思 考下面这个简单的例子。这里我们考虑把线性函数 $h(x) = cx$ 作为激活函数，把$y(x) = h(h(h(x)))$的运算对应3层神经网络A。这个运算会进行 $y(x) = c × c × c × x$ 的乘法运算，但是同样的处理可以由 $ y(x) = ax$ (注意， $a = c^3$)这一次乘法运算(即没有隐藏层的神经网络)来表示。如本例所示， 使用线性函数时，无法发挥多层网络带来的优势。因此，为了发挥叠加层所带来的优势，激活函数必须使用非线性函数。 

#### 实现Softmax函数时的注意事项

softmax 函数的实现中要进行指数函数的运算，但是此时指数函数的值很容易变得非常大。比如，$e^{10}$ 的值会超过 20000，$e^{100}$ 会变成一个后面有 40 多个 0 的超大值，$e^{1000}$ 的结果会返回一个表示无穷大的 inf。如果在这些超大值之间进行除法运算，结果会出现“不确定”的情况。

softmax 函数的实现可以像这样进行改进。

<img src="https://ws3.sinaimg.cn/large/006tNc79gy1fyya09c0kkj30m40c8dhb.jpg" width="400" />

在进行 softmax 的指数函数的运算时，加上(或者减去)某个常数并不会改变运算的结果。这里的$C^{\prime}$可以使用任何值，但是为了防止溢出，一般会使用输入信号中的最大值。

#### 神经网络的学习权重可以设置为0吗？

为什么不能将权重初始值设为 0 呢? 严格地说，为什么不能将权重初始值设成一样的值呢? 这是因为在误差反向传播法中，所有的权重值都会进行**相同的更新**。比如，在 2 层神经网络中，假设第 1 层和第 2 层的权重为 0。这样一来，正向传播时，因为输入层的权重为 0，所以第 2 层的神经元全部会被传递相同的值。第 2 层的神经元中全部输入相同的值，这意味着反向传播时第2层的权重全部都会进行相同的更新。因 此 ， 权 重 被 更 新 为 相 同 的 值 ， 并 拥 有 了 对 称 的 值 ( 重 复 的 值 )。 

这使得神经网络拥有许多不同的权重的意义丧失了。为了防止“权重均一化” (严格地讲，是为了瓦解权重的对称结构)，必须**随机生成初始值**。 