---
layout:     post
title:      "剑指机器学习--无监督学习"
subtitle:   机器学习方法复习笔记-5
date:       2019-04-04
author:     Lyon Ling
header-img: img/post-bg-jobhunting5.jpg
catalog: true
mathjax: true
tags:
    - JobHunting
    - MachineLearning
    - Notes
    - 百面机器学习
---

> 实际工作和现实场景中大部分数据是没有标签的，对于这种问题就需要模型算法去挖掘数据内在的结构和模式。
>
> 无监督学习主要包含两大学习方法：**数据聚类**和**特征变量关联**。其中数据聚类是通过多次迭代来找到最优的数据分割，特征变量关联是利用相关性分析的方法来找到变量之间的联系。

[TOC]

### 1. K-Means 均值聚类



### 2. GMM 高斯混合模型



### 3. SOM 自组织映射神经网络



### 4. 聚类算法的评估

#### 4.1 常见数据簇

相比监督学习，非监督学习没有数据标签，所以算法和模型直接影响了最终输出和模型性能。在评估不同聚类算法之前，需要先了解一下常见**数据簇(cluster)**的特点：

* 以中心定义的数据簇：这类数据倾向于球形分布，通常中心被定义为质心，即该数据簇所有点的平均值。集合中所有点到中心的距离比到其他簇中心的距离更近。
* 以密度定义的数据簇：这类数据呈现和周围数据簇明显不一样的数据密度，或稠密或稀疏。当数据簇不规则或者相互缠绕，并且有离群点和噪声点，常常使用基于密度的簇定义。
* 以连通定义的数据簇：这类数据集合中的数据点和数据点有连接关系，通常整个数据簇以图结构的形式呈现。该定义对不规则形状和相互缠绕的数据簇有效。
* 以概念定义的数据簇：这类数据集合中的所有点具有某些相同的性质。

聚类评估的任务是估计这数据集上进行聚类的可行性和聚类方法产生结果的质量，通常分为三个子任务：**估计聚类趋势**，**判定聚类簇数**，**判定聚类质量**。

#### 4.2 估计聚类趋势

这一步骤是检测数据集中是否有非随机的簇结构。如果数据基本是随机分布的，那么聚类的结果毫无意义。

简单来说可以通过观察**聚类误差**是否随着**聚类类别的数目**的增加而单调变化，如果数据是随机的，即不存在非随机的簇结构，那么聚类误差随着聚类类别数目变化的幅度应该不明显，并且也找不到一个可以表示数据真实簇数量的K值。

此外，可以用**霍普金斯统计量**(Hopkins Statistic)来判断数据在空间上的随机性。首先在样本总体中随机找n个点，记作$p_1,p_2,\dots,p_n$，对其中每个点$p_i$在样本空间里找到距离它最近的点，计算之间的距离，记作$x_i$，从而得到距离向量$x_1,x_2,\dots,x_n$；然后在样本可能的取值范围内随机生成n个点，记作$q_1,q_2,\dots,q_n$，对其中每个点$q_i$在样本空间里找到距离它最近的点，计算之间的距离，记作$y_i$，从而得到距离向量$y_1,y_2,\dots,y_n$。霍普金斯统计量$H$可以表示为：

$$
H = \frac{\sum_{i=1}^ny_i}{\sum_{i=1}^nx_i+\sum_{i=1}^ny_i}
$$
如果样本接近随机分布，那么$\sum^n_{i=1}y_i$和$\sum^n_{i=1}x_i$的值应该比较接近，即$H$的值应该接近0.5；如果聚类趋势明显，那么生成的样本点距离应该远大于实际样本点距离，即$\sum^n_{i=1}y_i \gg\sum^n_{i=1}x_i$，$H$的值应该接近1。

#### 4.3 判定聚类簇数

确定聚类趋势之后，需要找到与真实数据分布最接近的簇数，据此判断聚类的质量。判断方法很多，如[**手肘法**](http://www.scutmath.com/k_means_choose_k.html)，**Gap Statistic方法**。但是预估的最佳聚类簇数和程序输出的聚类簇数有时候是不一样的，有些算法，如DBSCAN，可以自动判断聚类的簇数，这就可能和通过其他方法判定的最优聚类簇数有所区别。

#### 4.4 判定聚类质量

在无监督的情况下，可以通过检查簇的分离情况和紧凑情况来评估聚类的效果。

* **平均轮廓系数**(Average Silhouette Coefficient)

  给定一个点$p$，该点的轮廓系数为

  $$
  s(p)=\frac{b(p)-a(p)}{max(a(p),b(p))}
  $$
  其中，$a(p)$是点$p$与同一个簇中所有其他店$p'$的平均距离，$b(p)$是点$p$与另一个簇中所有点的最小平均距离（如果有$n$个簇，那么只考虑平均距离最小的那个簇）。$a(p)$反映了点$p$所在簇的紧凑程度，$b(p)$ 反映了点$p$所在簇和其他簇的离散程度。所以$a(p)$越小，$b(p)$越大则说明聚类的效果越好。

  因此，用所有样本点的平均轮廓系数来表征聚类结果的质量：

  $$
  S(p)=\frac{1}{n}\sum_{i=1}^n\frac{b(p_i)-a(p_i)}{max(a(p_i),b(p_i))}
  $$

* **均方根标准偏差**(Root-mean-square standard deviation, RMSSTD)

  通常用来衡量聚类结果的同质性，即紧凑程度：

  $$
  RMSSTD=\big(\frac{\sum_i\sum_{x\in C_i}\|x-c_i\|^2}{P\sum_i(n_i-1)}\big)^{\frac{1}{2}}
  $$
  其中，$C_i$表示第$i$个簇，$c_i$表示该簇的中心，$n_i$表示该簇中的样本数量，$P$为样本点对应的向量维度。可以看出在分母上对向量的维度做了惩罚，因为样本点向量维度越高，整体平方距离的量度值会越大。 $\sum_i(n_i-1)=n-NC$中$n$为样本点总数，$NC$为聚类簇的个数，正常情况下$NC\ll n$，因此$\sum_i(n_i-1)$接近样本点的总数，为一个常数，综上可以把RMSSTD看作一个归一化的标准差。

* **R方**(R-Square)

  用来衡量聚类的差异度:

  $$
  RS=\frac{\sum_{x\in D}\|x-c\|^2-\sum_i\sum_{x\in C_i}\|x-c_i\|^2}{\sum_{x\in D}\|x-c\|^2}
  $$
  其中，$D$表示整个数据集，$c$表示$D$的中心点，所以$\sum_{x\in D}\|x-c\|^2$表示把整个数据集看成一类时的误差平方和，和RMSSTD相同$\sum_i\sum_{x\in C_i}\|x-c_i\|^2$代表将数据聚类后的误差平方和。所以$RS$代表聚类前后对应平方误差指标的改进幅度。

* 改进的**Hubert$\Gamma$统计**

  通过数据对的不一致性来评估聚类的差异：

  $$
\Gamma=\frac{2}{n(n-1)}\sum_{x\in D}\sum_{y\in D}d(x,y)d_{x\in C_i, y\in C_j}(c_i, c_j)
  $$
  其中，$d(x,y)$表示点$x$，$y$之间的距离， $d_{x\in C_i, y\in C_j}(c_i, c_j)$表示点$x$所在簇中心$c_i$到点$y$所在簇中心$c_j$的距离。$\frac{n(n-1)}{2}$表示所有$(x,y)$点对的个数，即最后对每个点的和做了归一化。理想情况下，对$(x,y)$，如果$d(x,y)$越小，对应的$d_{x\in C_i, y\in C_j}(c_i, c_j)$也应该越小，尤其是当$x$，$y$属于同一个聚类时，$d_{x\in C_i, y\in C_j}(c_i, c_j)=0$。所以$\Gamma$值越大，说明聚类结果和样本原始距离越吻合，即聚类的质量越高。

需要注意的是，为了更加合理地判断聚类算法的效果，还需要使用不同分布的数据集进行测试以观察算法的综合表现。