---
layout:     post
title:      "剑指机器学习--前向神经网络"
subtitle:   机器学习方法复习笔记-6
date:       2019-07-11
author:     Lyon Ling
header-img: img/post-bg-jobhunting5.jpg
catalog: true
mathjax: true
tags:
    - JobHunting
    - MachineLearning
    - Notes
    - 百面机器学习
---

> 深度前向神经网络（Deep Feedforward Network）是一种非常典型的深度学习模型。其目标就是为了拟合映射$y=f(x;\theta)$将输入的$x$转化为某种预测的输出$y$，并同时学习网络的参数$\theta$，使得模型能够达到最优的近似。
>
> 前向神经网络通常是多个函数复合表示，整体模型与一个有向无环图相关联，实际常用”链式法则“来表示函数的复合方式。常见的多层感知机，自编码器，以及卷积神经网络都是其中的一员。

[TOC]

### 1. 多层感知机和布尔函数



### 2. 深度网络中的激活函数

#### 2.1 常用的激活函数及其导数



#### 2.2 为什么Sigmoid和Tanh激活函数会导致梯度消失现象



### 3. 多层感知机的反向传播算法



### 4. 神经网络训练技巧

在训练大规模神经网络的时候, 我们常常遇到过拟合的问题. 为了解决过拟合, 我们常常使用

* 数据增强(Data Augmentation)
* 正则化(Regularization)
* 集成训练(Model Ensemble)
* 批量归一化(Batch Normalization)
* Dropout

#### 4.1 网络训练的时候是否可以初始化全部参数为0?

不可以.

考虑fully connected的深度网络, 同一层的任意神经元都是同构的, 并且拥有相同的输入和输出. 如果再将参数都初始化为相同的值, 那么无论前向传播还是反向传播, 取值都是完全相同的. 学习的过程没有办法打破这种对称性, 最后导致训练完成同一层下的所有参数还是相同的.

所以, 要随机初始化神经网络的参数的值, 以打破对称性. 简单来说, 可以初始化参数范围 $(-\frac{1}{\sqrt{d}},\frac{1}{\sqrt{d}})$的均匀分布, 其中 $d$ 是神经元输入数据的维度. 而Bias可以被简单设置为0, 并不会造成参数不对称的问题.

#### 4.2 为什么Dropout可以一直过拟合?



#### 4.3 Batch Norm的基本原理是什么? 在卷积网络中如何使用?



### 5. 卷积神经网络



#### 5.1 卷积的操作的本质特性是什么? 

稀疏交互(Sparse Interaction)和参数共享(Parameter Sharing).

* **稀疏交互**


* **参数共享**



#### 5.2 常用的池化操作有哪些? 池化的作用?



#### 5.3 卷积神经网络在文本分类任务中怎么使用?



### 6. 残差网络